{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized Tree Ensemble Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Required Packages\n",
    "import sklearn\n",
    "import statistics\n",
    "import scipy as sc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import cvxpy as cp\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Pool\n",
    "import time\n",
    "import re\n",
    "from sklearn import preprocessing\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "import mosek\n",
    "import statsmodels.api as sm\n",
    "\n",
    "def converge_test(sequence, threshold,length):\n",
    "    diff = np.diff(sequence)\n",
    "    if len(diff) < (length+1):\n",
    "        return False\n",
    "    else:\n",
    "        return ( max(np.abs(diff[-length:])) < threshold)\n",
    "    \n",
    "def compute_loss(y, y_hat): \n",
    "    return ((y - y_hat) ** 2) / 2\n",
    "\n",
    "def loss_gradient(y, y_hat): \n",
    "    return -(y-y_hat) \n",
    "\n",
    "def plot_tradeoff_curve(test_acc,nonzero,color,label):\n",
    "    results = pd.DataFrame()\n",
    "    for i in range(0,len(test_acc)):\n",
    "        results = results.append(pd.DataFrame(np.column_stack((test_acc[i],nonzero[i])),columns = ['test_acc','nonzero']))\n",
    "    agg = results.groupby(['nonzero'], as_index=False).agg({'test_acc':['mean','std','count']})\n",
    "    \n",
    "    plt.plot(agg['nonzero'],agg['test_acc']['mean'],color = color,label = label)\n",
    "    plt.errorbar(agg['nonzero'],agg['test_acc']['mean'], agg['test_acc']['std'],color = color)\n",
    "    plt.xlabel('Number of Nonzero Features')\n",
    "    plt.ylabel('Test Error')\n",
    "    plt.legend()\n",
    "\n",
    "def build_trees_bag(arg):\n",
    "    xTrain = arg[0]\n",
    "    yTrain = arg[1]\n",
    "    xTest= arg[2]\n",
    "    yTest= arg[3]\n",
    "    max_depth= arg[4]\n",
    "    lambd= arg[5]\n",
    "    threshold= arg[6] \n",
    "    sketch = arg[7]\n",
    "    problem_type = arg[8]\n",
    "    #not used\n",
    "    learning_rate = arg[9]\n",
    "    n_estimators = arg[10]\n",
    "    \n",
    "    train = xTrain\n",
    "    train = train.reset_index().drop('index',axis = 1)\n",
    "    train['yTrain'] = list(yTrain)\n",
    "\n",
    "    features = xTrain.columns\n",
    "    nfeatures = len(features)\n",
    "    importance_key = pd.DataFrame(features,columns = ['Features'])\n",
    "    \n",
    "    tree_results = []\n",
    "    i = 0\n",
    "    depth = 1\n",
    "    total_trees = 0\n",
    "    \n",
    "    for depth in range (1,max_depth+1):\n",
    "        i = 0\n",
    "        ### Early Stopping\n",
    "        early_stop_pred = []\n",
    "        early_stop_train_err = []\n",
    "        converged = False\n",
    "        \n",
    "        while converged == False:\n",
    "            train1 = train.sample(n = len(train), replace = True)\n",
    "        \n",
    "            yTrain1 = train1['yTrain']\n",
    "            xTrain1 = train1[features]\n",
    "            \n",
    "            if problem_type == 'Regression':\n",
    "                clf = DecisionTreeRegressor(max_depth = depth)\n",
    "            elif problem_type == 'Classification':\n",
    "                clf = DecisionTreeClassifier(max_depth = depth)\n",
    "        \n",
    "            clf.fit(xTrain1,yTrain1)\n",
    "            \n",
    "            imp = pd.DataFrame(np.column_stack((xTrain1.columns,clf.feature_importances_)), columns = ['Features','Importances'])\n",
    "            used = imp[imp['Importances']>0]['Features'].values\n",
    "            feature_indicator = [int(x in used) for x in features]\n",
    "            pred = clf.predict(xTrain[features])\n",
    "            feature_importances = pd.merge(importance_key,imp, on = 'Features', how = 'left').fillna(0)['Importances'].values\n",
    "            test_pred = clf.predict(xTest[features])\n",
    "            tree_results.append([pred,feature_indicator,feature_importances, test_pred  ,clf,xTrain1,yTrain1,features])\n",
    "            i = i+1\n",
    "            total_trees = total_trees+1\n",
    "            early_stop_pred.append(pred)\n",
    "            early_stop_train_err.append(np.sqrt(np.mean((np.mean(early_stop_pred,axis = 0) - yTrain)**2)))\n",
    "            converged = converge_test(early_stop_train_err,10**-2,3)\n",
    "    \n",
    "    return tree_results\n",
    "\n",
    "def solve_step_nonsketch(arg, tree_results):\n",
    "    \n",
    "    xTrain = arg[0]\n",
    "    yTrain = arg[1]\n",
    "    xTest= arg[2]\n",
    "    yTest= arg[3]\n",
    "    max_depth= arg[4]\n",
    "    lambd= arg[5]\n",
    "    threshold= arg[6] \n",
    "    sketch = arg[7]\n",
    "    problem_type = arg[8]\n",
    "    learning_rate = arg[9]\n",
    "    n_estimators = arg[10]\n",
    "    feature_list = xTrain.columns\n",
    "    \n",
    "    tree_pred = np.transpose(np.array([np.array(row[0]) for row in tree_results]))\n",
    "    test_pred = np.transpose(np.array([np.array(row[3]) for row in tree_results]))\n",
    "    indicators = np.transpose(np.array([np.array(row[1]) for row in tree_results]))\n",
    "    w = cp.Variable(len(tree_results),nonneg=True)\n",
    "    \n",
    "    if problem_type == 'Regression':\n",
    "        \n",
    "        objective = 0.5 * (1/len(yTrain))*cp.sum_squares(cp.matmul(tree_pred,w)-yTrain) + lambd*cp.norm(cp.matmul(indicators,w),1)\n",
    "        prob = cp.Problem(cp.Minimize(objective) )\n",
    "        prob.solve(solver = cp.MOSEK,mosek_params = {mosek.dparam.optimizer_max_time: 100.0} )\n",
    "        weights = np.asarray(w.value)\n",
    "        low_values_flags = np.abs(weights) < threshold  \n",
    "        weights[low_values_flags] = 0 \n",
    "\n",
    "        tree_ind = np.where(weights >0)[0]\n",
    "        if len(tree_ind)==0:\n",
    "            train_error =  np.sqrt(np.mean((yTrain )**2))\n",
    "            test_error = np.sqrt(np.mean((yTest )**2))\n",
    "            return([[],test_error,0,train_error])\n",
    "\n",
    "        importances = np.array([np.array(row[2]) for row in tree_results])\n",
    "        feature_importances = np.mean(importances[tree_ind],axis = 0)\n",
    "        nonzero_features = xTrain.columns[np.where(feature_importances >0)[0]]\n",
    "    \n",
    "        rf = RandomForestRegressor(n_estimators = 100).fit(xTrain[nonzero_features],yTrain)\n",
    "        test_pred = rf.predict(xTest[nonzero_features])\n",
    "        train_pred = rf.predict(xTrain[nonzero_features])\n",
    "    \n",
    "        train_error =  np.sqrt(np.mean((yTrain -train_pred)**2))\n",
    "        test_error = np.sqrt(np.mean((yTest -test_pred)**2))\n",
    "\n",
    "        return([feature_importances,test_error,len(nonzero_features),train_error])\n",
    "    \n",
    "    if problem_type == 'Classification':\n",
    "        \n",
    "        loss = -cp.sum(cp.multiply(yTrain, tree_pred@ w) - cp.logistic(tree_pred @ w))\n",
    "        objective =  (1/len(yTrain))*loss + lambd*cp.norm(cp.matmul(indicators,w),1)\n",
    "        prob = cp.Problem(cp.Minimize(objective) )\n",
    "        prob.solve(solver = cp.MOSEK ,mosek_params = {mosek.dparam.optimizer_max_time: 100.0})\n",
    "        weights = np.asarray(w.value)\n",
    "        low_values_flags = np.abs(weights) < threshold  # Where values are low\n",
    "        weights[low_values_flags] = 0\n",
    "        tree_ind = np.where(weights >0)[0]\n",
    "\n",
    "        if len(tree_ind)==0:\n",
    "            train_error =  np.sqrt(np.mean((yTrain )**2))\n",
    "            test_error = np.sqrt(np.mean((yTest )**2))\n",
    "            return([[],test_error,0,train_error])\n",
    "\n",
    "        importances = np.array([np.array(row[2]) for row in tree_results])\n",
    "        feature_importances = np.mean(importances[tree_ind],axis = 0)\n",
    "        nonzero_features = xTrain.columns[np.where(feature_importances >0)[0]]\n",
    "    \n",
    "        rf = RandomForestClassifier(n_estimators = 100).fit(xTrain[nonzero_features],yTrain)\n",
    "        \n",
    "        test_pred = rf.predict_proba(xTest[nonzero_features])[:,1]\n",
    "        train_pred = rf.predict_proba(xTrain[nonzero_features])[:,1]\n",
    "\n",
    "        train_error =  0#np.mean(yTrain != train_pred)\n",
    "        test_error = sklearn.metrics.roc_auc_score(yTest,test_pred)\n",
    "\n",
    "        return([feature_importances,test_error,len(nonzero_features),train_error])\n",
    "    \n",
    "def sparse_tree_ensemble(arg):\n",
    "    build_tree_method = arg[-1]\n",
    "    tree_results = build_tree_method(arg)\n",
    "    return solve_step_nonsketch(arg,tree_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(arg,ntrials,nfeatures,threshold):\n",
    "    pool = Pool(mp.cpu_count())\n",
    "    test_error_result = []\n",
    "    nonzero_result = []\n",
    "    train_error_result = []\n",
    "    count_array = []\n",
    "  \n",
    "    #bisection function\n",
    "    LL = 0\n",
    "    RL = 1\n",
    "    to_find = 0\n",
    "    counter = 0\n",
    "    while to_find <= nfeatures:\n",
    "        try:\n",
    "            counter = counter + 1 \n",
    "            trial = 0\n",
    "            result1= []\n",
    "            arg_list = []\n",
    "            lambd = (LL + RL)/2\n",
    "            while trial < ntrials:\n",
    "                arg[5] = lambd\n",
    "                arg_list.append(arg)\n",
    "                result = sparse_tree_ensemble(arg)\n",
    "                result1.append(result)\n",
    "                trial = trial + 1\n",
    "            #result1 = pool.map(sparse_tree_ensemble, arg_list)\n",
    "            test_acc = [row[1] for row in result1]\n",
    "            nonzero = [row[2] for row in result1]\n",
    "\n",
    "            train_acc = [row[3] for row in result1]\n",
    "\n",
    "            test_error_result.append(test_acc)\n",
    "            nonzero_result.append(nonzero)\n",
    "            count_array = np.append(count_array,nonzero)\n",
    "\n",
    "            train_error_result.append(train_acc)\n",
    "\n",
    "            freq = pd.DataFrame(np.column_stack(np.unique(count_array, return_counts = True)),columns = ['value','counts'])\n",
    "            print(freq.head(10))\n",
    "\n",
    "            mode_feature_current = np.mean(nonzero)\n",
    "\n",
    "            count_to_find = freq.loc[freq['value']==to_find]['counts'].values\n",
    "            print(to_find,mode_feature_current,lambd)\n",
    "\n",
    "            if len(count_to_find) > 0:\n",
    "                if count_to_find > threshold:\n",
    "                    RL = lambd\n",
    "                    LL = 0\n",
    "                    to_find = to_find + 1\n",
    "                    counter = 0\n",
    "\n",
    "                elif np.abs(mode_feature_current-to_find) >= 1:\n",
    "                    if mode_feature_current < to_find:\n",
    "                        RL = lambd \n",
    "                    else:\n",
    "                        LL = lambd\n",
    "\n",
    "            elif mode_feature_current < to_find:\n",
    "                RL = lambd\n",
    "            elif mode_feature_current >= to_find:\n",
    "                LL = lambd\n",
    "                \n",
    "            elif counter >= 10:\n",
    "                    RL = lambd\n",
    "                    LL = 0\n",
    "                    to_find = to_find + 1\n",
    "                    counter = 0\n",
    "\n",
    "            print('counter:', counter)\n",
    "        except:\n",
    "            print('Solver Failed')\n",
    "        pool.close()\n",
    "    return test_error_result,nonzero_result,train_error_result\n",
    "\n",
    "def baseline(xTrain,yTrain,xTest,yTest,problem_type,nfeatures):\n",
    "    \n",
    "    if problem_type == 'Regression':\n",
    "        model = RandomForestRegressor(n_estimators = 100)\n",
    "    if problem_type == 'Classification':\n",
    "            model = RandomForestClassifier(n_estimators = 100)\n",
    "    \n",
    "    ### RF feature select baseline\n",
    "    rf = model.fit(xTrain,yTrain)\n",
    "    imp = pd.DataFrame(np.column_stack((xTrain.columns,rf.feature_importances_)),columns = ['features','scores']).sort_values('scores',ascending = False)\n",
    "    print(imp)\n",
    "    acc =[]\n",
    "    n_features = []\n",
    "    se = []\n",
    "    for i in range(1,nfeatures):\n",
    "        to_use = imp.head(i)['features'].values\n",
    "        trial = 0\n",
    "        acc1 = []\n",
    "        while trial < 10:\n",
    "            rf1 = model.fit(xTrain[to_use],yTrain)\n",
    "            pred = rf1.predict_proba(xTest[to_use])[:,1]\n",
    "            \n",
    "            if problem_type == 'Regression':\n",
    "                acc1.append(np.sqrt(np.mean((yTest-pred)**2)))\n",
    "                \n",
    "            if problem_type == 'Classification':\n",
    "                acc1.append(sklearn.metrics.roc_auc_score(yTest,pred))\n",
    "                \n",
    "            trial = trial+1\n",
    "            \n",
    "        acc.append(np.mean(acc1))\n",
    "        se.append(np.std(acc1))\n",
    "        n_features.append(i)\n",
    "    return acc,n_features,se"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
